{"metadata":{"colab":{"provenance":[{"file_id":"1864-SNFZyv9ZW1Rlp6E7JwYYY4cMyU2w","timestamp":1724002010782},{"file_id":"1PZ4GQd7d2bpXrk6YPLsl-SyN6PUdmT4f","timestamp":1723993665320},{"file_id":"1R36KH4f6qjOfBzAkXomGe-98Z5LiJgsv","timestamp":1723993376393},{"file_id":"1I2VWMlKEQ5M62FrLbxOal49FTvL0V0qS","timestamp":1723978471053},{"file_id":"1UcSf1W0yOLjQIxb6FA_JAiFHUAh8-FRN","timestamp":1716648186776}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9199144,"sourceType":"datasetVersion","datasetId":5561622},{"sourceId":3346317,"sourceType":"datasetVersion","datasetId":1935874}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/prathamkumar0011/pest-classification-detection?scriptVersionId=193851386\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# **Welcome to the Ali Analytics recruitment test.**\n\n\n---\n\n\n\n**Problem Statement:**\n\n*  The agricultural industry faces significant challenges due to insect pests, which can cause substantial crop damage and yield loss. Accurate and timely identification of these pests is crucial for effective pest management and control.\n\n**Dataset:**\n\n-  The IP102 dataset provides a large-scale benchmark for insect pest recognition, containing over 75,000 images across 102 categories (classes)\n-  The dataset has a split of 6:1:3 highly imbalanced some of the class consists of 71 images while the largest is 5740.\n\nThe dataset comprises:\n\n    - 75,000 Images\n    - 102 classes\n    - 45,095 images in the training set\n    - 22,169 image samples in the test\n\n- **GitHub link:** https://github.com/xpwu95/IP102\n  - trainset: https://drive.google.com/drive/folders/1UPW_wyfn-oP6YyO_AnUgue7Gur2dYcxP?usp=drive_link\n  - valset: https://drive.google.com/drive/folders/1JddZOkBMRnfJHzRCSJS26JVbbG989W_L?usp=drive_link\n  - testset: https://drive.google.com/drive/folders/1lWQwYxrYJ1ZfZrmQg51rnNRgl1InLWH0?usp=drive_link\n\n\n**Further instructions:**\n- Framework: TensorFlow 2.x / Pytorch\n- Programing language: Python 3.x\n\n**Submission Guidelines:**\n*   Submit your solution as a notebook(ipynb) or Python script (.py)\n*   Include clear, well-documented code and concise explanations for each step.\n*   Trained Model: Capable of accurately classifying insect pests.\n*   Performance Metrics: Demonstrating the effectiveness of the model.\n*   Detailed Report: Summarizing the process, challenges, solutions, and insights.\n*   Presentation: Effectively communicating the findings and results.\n*   Email your submission to ali.analytics.io@gmail.com by the **19th of August, 2024, 21:00 IST**\n\n\n**Evaluation Criteria:**\n*   Code quality and documentation\n*   Trained Model and Model performance and accuracy\n*   Approach to problem-solving and data analysis\n*   Project Presentation skills\n\n\n**To assess your competence, we have divided the project into distinct steps, each with specific tasks. Please follow the steps below to complete the task:**","metadata":{"id":"NdKqQRvkRIc7"}},{"cell_type":"markdown","source":"# **Step 1: Environment Setup**","metadata":{"id":"_cnpRgl7AaEI"}},{"cell_type":"markdown","source":"**Tasks:**\n\n1. Set up a Google Colab environment for the project.\n\n  *   Install the necessary libraries (TensorFlow 2.x, Pytorch, NumPy, OpenCV, Matplotlib, scikit-learn etc.).\n  *   Verify the installation by printing the versions of the installed libraries.\n2. Add anything else that is required for this step (optional)\n\n","metadata":{"id":"wwJhTVdiAhZ2"}},{"cell_type":"markdown","source":"# **Step 2: Data Preparation**","metadata":{"id":"VaT-H5RTArvO"}},{"cell_type":"markdown","source":"Tasks:\n\n1. Use Github/Google drive to download the IP102 dataset\n2. Load the images and/or annotations into the environment.\n3. Data Cleaning: Handle missing or inconsistent data.\n4. Data Augmentation: Apply techniques such as rotation, flipping, and scaling to address class imbalance.\n5. Add anything else that is required for this step (optional)","metadata":{"id":"1i7fuiz8BAof"}},{"cell_type":"markdown","source":"# **Step 3: Exploratory Data Analysis (EDA)**","metadata":{"id":"wm7v5sP2C3u_"}},{"cell_type":"markdown","source":"**Tasks:**\n\n1. Analyze and visualize the distribution of object classes.\n2. Visualize sample images\n3. Identify any missing values\n4. Add anything else that is required for this step (optional)","metadata":{"id":"1EYXVl5RC7KD"}},{"cell_type":"markdown","source":"# **Step 4: Feature Engineering**","metadata":{"id":"i1e7RO3e79hv"}},{"cell_type":"markdown","source":"**Objective:** Create meaningful features that enhance model performance.\n\n**Tasks:**\n\n1. Feature Extraction: Experiment with different feature extraction techniques such as Histogram of Oriented Gradients (HOG), Scale-Invariant Feature Transform (SIFT), or deep learning-based features.\n2. Feature Selection: Select the most relevant features that contribute to model accuracy.\n3. Add anything else that is required for this step (optional)","metadata":{"id":"GcmYoWpc8MBE"}},{"cell_type":"markdown","source":"# **Step 5: Model Training**","metadata":{"id":"QQbLMxp-DGq3"}},{"cell_type":"markdown","source":"**Objective:** Develop and train machine learning models for pest classification\n\n**Tasks:**\n\n1. **Data Splitting:** Split the dataset into training, validation, and test sets.\n2. **Model Training:** Train multiple models (e.g., Convolutional Neural Networks, Random Forest, Support Vector Machines, ..) for classification.\n3. **Object Detection:** Implement object detection models (e.g., YOLO, Faster R-CNN, VGG16, ..) for images with bounding box annotations.\n4. Add anything else that is required for this step (optional)\n\n**Motivation:** The use of multiple models and object detection techniques can enhance the robustness of the solution","metadata":{"id":"H36347lyDKpx"}},{"cell_type":"markdown","source":"# **Step 6: Model Evaluation and Tuning**","metadata":{"id":"qkG5WxemDYgZ"}},{"cell_type":"markdown","source":"**Objective:** Evaluate and optimize model performance.\n\n**Tasks:**\n\n1. Evaluate the model on the validation/test set using appropriate metrics (e.g., precision, recall, mAP).\n2. Visualize detection results on sample test images.\n3. Discuss the model's performance and potential improvements.\n4. Hyperparameter Tuning: Perform hyperparameter tuning to optimize model performance.\n5. Model Comparison: Compare the performance of different models and select the best-performing one.\n6. Add anything else that is required for this step (optional)\n\n**Motivation:** Proper evaluation and tuning are essential for achieving high model accuracy.","metadata":{"id":"lYxOsNYvDbF1"}},{"cell_type":"markdown","source":"# **Step 7: Deployment**","metadata":{"id":"xYRPzF4kDgn7"}},{"cell_type":"markdown","source":"**Tasks:**\n\n1. Explain how you would deploy your trained object detection model in a production environment.\n2. Discuss the different deployment options available and their pros and cons.\n3. Based on the deployment options, which deployment option would you go for and why?\n3. Outline the steps you would take to monitor the deployed model and ensure its performance over time.\n4. Add anything else that is required for this step (optional)","metadata":{"id":"FqajkYkIDuHH"}},{"cell_type":"markdown","source":"# **Good luck, and we look forward to reviewing your submission!**","metadata":{"id":"2BjllpAnD0w1"}},{"cell_type":"markdown","source":"# **Step 1: Environment Setup**","metadata":{"id":"wr5OsmUDhQnl"}},{"cell_type":"code","source":"!pip install -q tensorflow matplotlib numpy scikit-learn torch opencv-python wurlitzer","metadata":{"id":"kJYzwq3vhP3k","executionInfo":{"status":"ok","timestamp":1724004939740,"user_tz":-330,"elapsed":3900,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"854c652e-753c-4a11-bf8a-43cf3e013aaa","execution":{"iopub.status.busy":"2024-08-19T06:35:50.380427Z","iopub.execute_input":"2024-08-19T06:35:50.381102Z","iopub.status.idle":"2024-08-19T06:36:03.018272Z","shell.execute_reply.started":"2024-08-19T06:35:50.38107Z","shell.execute_reply":"2024-08-19T06:36:03.017234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q torch torchvision torchaudio","metadata":{"execution":{"iopub.status.busy":"2024-08-19T06:35:27.287011Z","iopub.execute_input":"2024-08-19T06:35:27.287879Z","iopub.status.idle":"2024-08-19T06:35:39.358893Z","shell.execute_reply.started":"2024-08-19T06:35:27.287841Z","shell.execute_reply":"2024-08-19T06:35:39.35788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"Z_D7HEBihtg-","executionInfo":{"status":"ok","timestamp":1724002147256,"user_tz":-330,"elapsed":33991,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"61906763-a3df-44da-a797-fc098d634203"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nfrom sklearn.utils.class_weight import compute_class_weight","metadata":{"id":"KQOp0YOEiJNb","executionInfo":{"status":"ok","timestamp":1724003224903,"user_tz":-330,"elapsed":4416,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"execution":{"iopub.status.busy":"2024-08-19T06:36:09.701618Z","iopub.execute_input":"2024-08-19T06:36:09.702484Z","iopub.status.idle":"2024-08-19T06:36:23.429003Z","shell.execute_reply.started":"2024-08-19T06:36:09.702425Z","shell.execute_reply":"2024-08-19T06:36:23.428001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 2: Data Preparation**","metadata":{"id":"wdV6RW4tlClW"}},{"cell_type":"markdown","source":"## Data loading","metadata":{"id":"yaX7zHNNH6kA"}},{"cell_type":"code","source":"# import tarfile\n\n# tar_file_path = '/kaggle/input/ip102data/ip102_v1.1.tar'\n\n# with tarfile.open(tar_file_path, 'r') as tar:\n#     tar.extractall(path='/content')","metadata":{"id":"T9P8k3o8m5Z1","executionInfo":{"status":"ok","timestamp":1724003631574,"user_tz":-330,"elapsed":52943,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"execution":{"iopub.status.busy":"2024-08-18T21:01:40.500787Z","iopub.execute_input":"2024-08-18T21:01:40.501564Z","iopub.status.idle":"2024-08-18T21:01:40.505462Z","shell.execute_reply.started":"2024-08-18T21:01:40.501532Z","shell.execute_reply":"2024-08-18T21:01:40.504419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\nimport os\n\n# Define paths\nbase_dir = '/kaggle/input/ip102data/ip102_v1.1/images'\noutput_dir = '/kaggle/working/ip102_v1.1/data'\ntrain_file = '/kaggle/input/ip102data/ip102_v1.1/train.txt'\ntest_file = '/kaggle/input/ip102data/ip102_v1.1/test.txt'\nval_file = '/kaggle/input/ip102data/ip102_v1.1/val.txt'","metadata":{"id":"FkQaxNJEIHCf","execution":{"iopub.status.busy":"2024-08-19T06:36:23.430644Z","iopub.execute_input":"2024-08-19T06:36:23.431135Z","iopub.status.idle":"2024-08-19T06:36:23.435934Z","shell.execute_reply.started":"2024-08-19T06:36:23.43111Z","shell.execute_reply":"2024-08-19T06:36:23.435003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create output directories based on label\ndef create_label_dirs(base_path, labels):\n    for label in set(labels):\n        os.makedirs(os.path.join(base_path, str(label)), exist_ok=True)\n\n# Parse file and copy images\ndef process_file(file_list, base_dir, output_dir):\n    labels = []\n    with open(file_list, 'r') as file:\n        lines = file.read().splitlines()\n        for line in lines:\n            img, label = line.split()\n            labels.append(label)  # Collect labels first\n\n    # Create directories for labels\n    create_label_dirs(output_dir, labels)\n\n    # Copy images to corresponding directories\n    with open(file_list, 'r') as file:\n        lines = file.read().splitlines()\n        for line in lines:\n            img, label = line.split()\n            src = os.path.join(base_dir, img)\n            dest = os.path.join(output_dir, label, img)\n            if os.path.isfile(src):\n                shutil.copy(src, dest)\n            else:\n                print(f\"File not found: {src}\")\n\n# Execute the function for training, testing, and validation data\nprocess_file(train_file, base_dir, os.path.join(output_dir, 'train'))\nprocess_file(test_file, base_dir, os.path.join(output_dir, 'test'))\nprocess_file(val_file, base_dir, os.path.join(output_dir, 'val'))\n","metadata":{"id":"-NsGN8DRqLAd","executionInfo":{"status":"ok","timestamp":1724004684715,"user_tz":-330,"elapsed":59557,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"execution":{"iopub.status.busy":"2024-08-19T06:36:30.509334Z","iopub.execute_input":"2024-08-19T06:36:30.510282Z","iopub.status.idle":"2024-08-19T06:42:57.392467Z","shell.execute_reply.started":"2024-08-19T06:36:30.510246Z","shell.execute_reply":"2024-08-19T06:42:57.391587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{"id":"b_lwlW3GI1qM"}},{"cell_type":"code","source":"def check_duplicates(file_path):\n    with open(file_path, 'r') as file:\n        lines = file.read().splitlines()\n        seen = set()\n        duplicates = set()\n        for line in lines:\n            if line in seen:\n                duplicates.add(line)\n            seen.add(line)\n\n    if duplicates:\n        print(f\"Duplicate entries found in {file_path}: {duplicates}\")\n    else:\n        print(f\"No duplicate entries found in {file_path}.\")\n\n# Check for duplicates in each file\ncheck_duplicates(train_file)\ncheck_duplicates(test_file)\ncheck_duplicates(val_file)\n","metadata":{"id":"mDbu_yyquk6C","executionInfo":{"status":"ok","timestamp":1724005446693,"user_tz":-330,"elapsed":1111,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"c81967e0-d929-4041-f4a9-1cf379024630","execution":{"iopub.status.busy":"2024-08-19T06:43:13.61363Z","iopub.execute_input":"2024-08-19T06:43:13.6145Z","iopub.status.idle":"2024-08-19T06:43:13.648632Z","shell.execute_reply.started":"2024-08-19T06:43:13.61444Z","shell.execute_reply":"2024-08-19T06:43:13.647631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_missing_values(file_path):\n    missing_lines = []\n    with open(file_path, 'r') as file:\n        lines = file.read().splitlines()\n        for i, line in enumerate(lines):\n            if len(line.strip()) == 0:\n                missing_lines.append(i)\n                continue\n            parts = line.split()\n            if len(parts) != 2 or not parts[1].isdigit():\n                missing_lines.append(i)\n\n    if missing_lines:\n        print(f\"Missing or invalid entries found in {file_path} at lines: {missing_lines}\")\n    else:\n        print(f\"No missing or invalid entries found in {file_path}\")\n\n# Check for missing values in train.txt, test.txt, and val.txt\ncheck_missing_values(train_file)\ncheck_missing_values(test_file)\ncheck_missing_values(val_file)\n","metadata":{"id":"id6GvpRSt88w","executionInfo":{"status":"ok","timestamp":1724005266439,"user_tz":-330,"elapsed":625,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"b7c0c94f-17bb-4615-a8a9-9aedc00b0bc8","execution":{"iopub.status.busy":"2024-08-19T06:43:15.015887Z","iopub.execute_input":"2024-08-19T06:43:15.016246Z","iopub.status.idle":"2024-08-19T06:43:15.084391Z","shell.execute_reply.started":"2024-08-19T06:43:15.016217Z","shell.execute_reply":"2024-08-19T06:43:15.083528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_image_existence(file_path, base_dir):\n    missing_images = []\n    with open(file_path, 'r') as file:\n        lines = file.read().splitlines()\n        for line in lines:\n            img, _ = line.split()\n            img_path = os.path.join(base_dir, img)\n            if not os.path.isfile(img_path):\n                missing_images.append(img)\n\n    if missing_images:\n        print(f\"Missing images listed in {file_path}: {missing_images}\")\n    else:\n        print(f\"All images listed in {file_path} exist.\")\n\n# Check image existence for each file\ncheck_image_existence(train_file, base_dir)\ncheck_image_existence(test_file, base_dir)\ncheck_image_existence(val_file, base_dir)","metadata":{"id":"6fqMgwMLuMfY","executionInfo":{"status":"ok","timestamp":1724005358897,"user_tz":-330,"elapsed":1098,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"b06de72c-adc9-4197-d81e-b6326ac26d23","execution":{"iopub.status.busy":"2024-08-19T06:43:15.983921Z","iopub.execute_input":"2024-08-19T06:43:15.984542Z","iopub.status.idle":"2024-08-19T06:43:53.358891Z","shell.execute_reply.started":"2024-08-19T06:43:15.984508Z","shell.execute_reply":"2024-08-19T06:43:53.357846Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Augmentation","metadata":{"id":"GY_1KJ3BIM83"}},{"cell_type":"code","source":"#  Setting up directories\ntrain_dir = '/kaggle/working/ip102_v1.1/data/train'\nval_dir = '/kaggle/working/ip102_v1.1/data/test'\ntest_dir = '/kaggle/working/ip102_v1.1/data/val'\n\n# Image Preprocessing\nimg_size = (128, 128)\nbatch_size = 64","metadata":{"id":"-FF3MnQXLYjw","execution":{"iopub.status.busy":"2024-08-19T06:43:53.361465Z","iopub.execute_input":"2024-08-19T06:43:53.36173Z","iopub.status.idle":"2024-08-19T06:43:53.366189Z","shell.execute_reply.started":"2024-08-19T06:43:53.361707Z","shell.execute_reply":"2024-08-19T06:43:53.365257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data Augmentation for training set\ntrain_datagen = ImageDataGenerator(\n    rescale=1./255,\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\n# Only rescaling for validation and test sets\nval_test_datagen = ImageDataGenerator(rescale=1./255)","metadata":{"id":"u3wl_oyULfbh","execution":{"iopub.status.busy":"2024-08-19T06:43:53.367328Z","iopub.execute_input":"2024-08-19T06:43:53.367688Z","iopub.status.idle":"2024-08-19T06:43:53.37932Z","shell.execute_reply.started":"2024-08-19T06:43:53.367654Z","shell.execute_reply":"2024-08-19T06:43:53.378437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading data\ntrain_generator = train_datagen.flow_from_directory(\n    train_dir,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\nval_generator = val_test_datagen.flow_from_directory(\n    val_dir,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical'\n)\n\ntest_generator = val_test_datagen.flow_from_directory(\n    test_dir,\n    target_size=img_size,\n    batch_size=batch_size,\n    class_mode='categorical',\n    shuffle=False\n)","metadata":{"id":"ZZKpRgiolNXv","executionInfo":{"status":"ok","timestamp":1724004766979,"user_tz":-330,"elapsed":1574,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"2a5c99d5-867c-4505-f646-7d376a09c3f7","execution":{"iopub.status.busy":"2024-08-19T06:43:53.381388Z","iopub.execute_input":"2024-08-19T06:43:53.382009Z","iopub.status.idle":"2024-08-19T06:43:56.148091Z","shell.execute_reply.started":"2024-08-19T06:43:53.381984Z","shell.execute_reply":"2024-08-19T06:43:56.147203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compute class weights to handle imbalance\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_generator.classes),\n    y=train_generator.classes\n)\n\nclass_weights = {i: weight for i, weight in enumerate(class_weights)}\nprint(\"Class weights:\", class_weights)","metadata":{"id":"KLn2pP8nl-XA","executionInfo":{"status":"ok","timestamp":1724004854038,"user_tz":-330,"elapsed":658,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"3f440ea3-7669-420a-f7ab-ee89920b113a","execution":{"iopub.status.busy":"2024-08-19T06:43:56.149172Z","iopub.execute_input":"2024-08-19T06:43:56.149426Z","iopub.status.idle":"2024-08-19T06:43:56.212703Z","shell.execute_reply.started":"2024-08-19T06:43:56.149403Z","shell.execute_reply":"2024-08-19T06:43:56.211792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 3: Exploratory Data Analysis (EDA)**","metadata":{"id":"oD4oS4r5lIbV"}},{"cell_type":"code","source":"# Visualizing some images\ndef plot_images(images_arr):\n    fig, axes = plt.subplots(2, 5, figsize=(10, 8))\n    axes = axes.flatten()\n    for img, ax in zip(images_arr, axes):\n        ax.imshow(img)\n    plt.tight_layout()\n    plt.show()\n\nsample_training_images, _ = next(train_generator)\nplot_images(sample_training_images[:10])","metadata":{"id":"N92tHG-7lFtM","executionInfo":{"status":"ok","timestamp":1724006079000,"user_tz":-330,"elapsed":4739,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"9e84d081-e333-4eae-8b09-c9e82544303d","execution":{"iopub.status.busy":"2024-08-19T06:43:56.213914Z","iopub.execute_input":"2024-08-19T06:43:56.214332Z","iopub.status.idle":"2024-08-19T06:43:58.165295Z","shell.execute_reply.started":"2024-08-19T06:43:56.2143Z","shell.execute_reply":"2024-08-19T06:43:58.164381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 4: Feature Engineering**","metadata":{"id":"AHzaiD-BlQIP"}},{"cell_type":"code","source":"import cv2\n\ndef extract_sift_features(image_path):\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    sift = cv2.SIFT_create()\n    keypoints, descriptors = sift.detectAndCompute(image, None)\n    return keypoints, descriptors\n\n# Example usage\nkeypoints, descriptors = extract_sift_features('/content/ip102_v1.1/data/train/0/00002.jpg')\nprint(\"SIFT descriptors shape:\", descriptors.shape)","metadata":{"id":"HqmKejoGlUED","executionInfo":{"status":"ok","timestamp":1724006393615,"user_tz":-330,"elapsed":617,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"19b76eba-cc50-4114-9de6-db3fc9271538"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\nimport matplotlib.pyplot as plt\n\ndef extract_and_display_sift_features(image_path):\n    # Load the image in grayscale\n    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n\n    # Initialize SIFT detector\n    sift = cv2.SIFT_create()\n\n    # Detect keypoints and compute descriptors\n    keypoints, descriptors = sift.detectAndCompute(image, None)\n\n    # Convert keypoints to numpy array\n    keypoints_img = cv2.drawKeypoints(image, keypoints, None, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n\n    # Display the image with keypoints\n    plt.figure(figsize=(10, 10))\n    plt.imshow(cv2.cvtColor(keypoints_img, cv2.COLOR_BGR2RGB))\n    plt.axis('off')\n    plt.title('SIFT Keypoints')\n    plt.show()\n\nimage_path = '/kaggle/working/ip102_v1.1/data/train/0/00002.jpg'  \nextract_and_display_sift_features(image_path)\n","metadata":{"id":"FuUN7EGc0dIR","executionInfo":{"status":"ok","timestamp":1724006938721,"user_tz":-330,"elapsed":4321,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"73ba1cf3-b302-4d8a-a4ae-7f0ff23dbed9","execution":{"iopub.status.busy":"2024-08-19T06:46:49.209263Z","iopub.execute_input":"2024-08-19T06:46:49.210011Z","iopub.status.idle":"2024-08-19T06:46:49.583544Z","shell.execute_reply.started":"2024-08-19T06:46:49.209978Z","shell.execute_reply":"2024-08-19T06:46:49.582717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 5: Model Training**\n\n\n","metadata":{"id":"E0AZiiGslUhQ"}},{"cell_type":"code","source":"# CNN Model Architecture\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\ninput_shape=(128, 128, 3)\nmodel = Sequential([\n    Conv2D(128, (3, 3), input_shape=input_shape, activation='relu', padding='same'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Conv2D(64, (3, 3), activation='relu'),\n    MaxPooling2D(pool_size=(2, 2)),\n    Flatten(),\n    Dense(256, activation='relu'),\n    Dropout(0.25),\n    Dense(128, activation='relu'),\n    Dropout(0.25),\n    Dense(102, activation='softmax')\n])","metadata":{"id":"ebHY_aPelds7","executionInfo":{"status":"ok","timestamp":1724007193571,"user_tz":-330,"elapsed":664,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"execution":{"iopub.status.busy":"2024-08-19T06:47:26.053568Z","iopub.execute_input":"2024-08-19T06:47:26.054306Z","iopub.status.idle":"2024-08-19T06:47:27.084618Z","shell.execute_reply.started":"2024-08-19T06:47:26.054276Z","shell.execute_reply":"2024-08-19T06:47:27.083636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compiling the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Summary of the model\nmodel.summary()","metadata":{"id":"25uEW7t41N3e","executionInfo":{"status":"ok","timestamp":1724007239015,"user_tz":-330,"elapsed":634,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"ca531041-8f39-487f-bb54-be7d943fc33f","execution":{"iopub.status.busy":"2024-08-19T06:47:41.436664Z","iopub.execute_input":"2024-08-19T06:47:41.437333Z","iopub.status.idle":"2024-08-19T06:47:41.487226Z","shell.execute_reply.started":"2024-08-19T06:47:41.437298Z","shell.execute_reply":"2024-08-19T06:47:41.486354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# Early Stopping and Model Checkpoint\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\nmodel_checkpoint = ModelCheckpoint('best_model.keras', monitor='val_accuracy', save_best_only=True)\n\n# Training the model\nhistory = model.fit(\n    train_generator,\n    epochs=10,\n    validation_data=val_generator,\n    class_weight=class_weights,\n    callbacks=[early_stopping, model_checkpoint]\n)","metadata":{"id":"pRGL8dTA1wN5","executionInfo":{"status":"ok","timestamp":1724011150749,"user_tz":-330,"elapsed":3838123,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"f915429b-60d1-4e44-a0ab-ab389f162215","execution":{"iopub.status.busy":"2024-08-19T06:47:51.955028Z","iopub.execute_input":"2024-08-19T06:47:51.955675Z","iopub.status.idle":"2024-08-19T07:43:36.155414Z","shell.execute_reply.started":"2024-08-19T06:47:51.955642Z","shell.execute_reply":"2024-08-19T07:43:36.154649Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nhistory_df = pd.DataFrame(history.history)\nhistory_df","metadata":{"execution":{"iopub.status.busy":"2024-08-19T07:47:20.545071Z","iopub.execute_input":"2024-08-19T07:47:20.545929Z","iopub.status.idle":"2024-08-19T07:47:20.576297Z","shell.execute_reply.started":"2024-08-19T07:47:20.545895Z","shell.execute_reply":"2024-08-19T07:47:20.575442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"id":"esM4UT-E4gIs","executionInfo":{"status":"ok","timestamp":1724011156205,"user_tz":-330,"elapsed":1143,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"864d77f6-dfeb-4bb2-8aeb-25b38210a58c","execution":{"iopub.status.busy":"2024-08-19T07:47:27.894862Z","iopub.execute_input":"2024-08-19T07:47:27.895435Z","iopub.status.idle":"2024-08-19T07:47:28.172165Z","shell.execute_reply.started":"2024-08-19T07:47:27.895397Z","shell.execute_reply":"2024-08-19T07:47:28.171276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\nplt.show()","metadata":{"id":"B8a6CJD-4k70","executionInfo":{"status":"ok","timestamp":1724011157282,"user_tz":-330,"elapsed":5,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"55a86840-3ded-4d03-94c6-a7b4ebe042a1","execution":{"iopub.status.busy":"2024-08-19T07:47:33.652977Z","iopub.execute_input":"2024-08-19T07:47:33.653329Z","iopub.status.idle":"2024-08-19T07:47:33.876845Z","shell.execute_reply.started":"2024-08-19T07:47:33.653299Z","shell.execute_reply":"2024-08-19T07:47:33.875874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 6: Model Evaluation and Tuning**","metadata":{"id":"TZTMqXJTlhTN"}},{"cell_type":"code","source":"import shutil\n\nsource_path = 'best_model.keras' # Replace with the actual path\ndestination_path = '/kaggle/working/ip102_v1.1/model.keras' # Where you want to copy it\nshutil.copy(source_path, destination_path)","metadata":{"id":"7V51xshvFjmn","executionInfo":{"status":"ok","timestamp":1724011525197,"user_tz":-330,"elapsed":58059,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"1226b444-7325-40ed-e2ee-53f0a0793ce5","execution":{"iopub.status.busy":"2024-08-19T07:48:28.353016Z","iopub.execute_input":"2024-08-19T07:48:28.353384Z","iopub.status.idle":"2024-08-19T07:48:28.506435Z","shell.execute_reply.started":"2024-08-19T07:48:28.353346Z","shell.execute_reply":"2024-08-19T07:48:28.505513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\nimport seaborn as sns\n\n# Loading the best model\nmodel.load_weights('/kaggle/working/ip102_v1.1/model.keras')\n\n# Evaluate on test data\ntest_loss, test_accuracy = model.evaluate(test_generator)\nprint(f'Test accuracy: {test_accuracy*100:.2f}%')\n\n# Predicting the test data\npredictions = model.predict(test_generator)\npredicted_classes = np.argmax(predictions, axis=1)\n","metadata":{"id":"51yXLY_Xlmsu","executionInfo":{"status":"ok","timestamp":1724011858813,"user_tz":-330,"elapsed":81967,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"37aaa7ba-c1eb-4c0f-c5cd-9bb09b03f5c1","execution":{"iopub.status.busy":"2024-08-19T07:48:46.572073Z","iopub.execute_input":"2024-08-19T07:48:46.572585Z","iopub.status.idle":"2024-08-19T07:49:22.937528Z","shell.execute_reply.started":"2024-08-19T07:48:46.57255Z","shell.execute_reply":"2024-08-19T07:49:22.93665Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classification report\nreport = classification_report(test_generator.classes, predicted_classes, target_names=list(test_generator.class_indices.keys()))\nprint(report)","metadata":{"id":"OnZ275bQG7uB","executionInfo":{"status":"ok","timestamp":1724011760897,"user_tz":-330,"elapsed":967,"user":{"displayName":"Pratham Kumar","userId":"05692637074222750046"}},"outputId":"2aa1f040-c8d3-4f60-b77a-29373a975138","execution":{"iopub.status.busy":"2024-08-19T07:50:07.552302Z","iopub.execute_input":"2024-08-19T07:50:07.552684Z","iopub.status.idle":"2024-08-19T07:50:07.578426Z","shell.execute_reply.started":"2024-08-19T07:50:07.552657Z","shell.execute_reply":"2024-08-19T07:50:07.577431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **YOLO v5**","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:26:42.936733Z","iopub.execute_input":"2024-08-19T14:26:42.93749Z","iopub.status.idle":"2024-08-19T14:26:43.326356Z","shell.execute_reply.started":"2024-08-19T14:26:42.937456Z","shell.execute_reply":"2024-08-19T14:26:43.325577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!git clone https://github.com/ultralytics/yolov5  # clone\n%cd yolov5\n%pip install -qr requirements.txt  # install\n\nimport torch\nimport utils\ndisplay = utils.notebook_init()","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:26:43.328037Z","iopub.execute_input":"2024-08-19T14:26:43.328601Z","iopub.status.idle":"2024-08-19T14:27:30.264853Z","shell.execute_reply.started":"2024-08-19T14:26:43.328568Z","shell.execute_reply":"2024-08-19T14:27:30.26374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%cd '/kaggle/working/yolov5'","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:27:30.267053Z","iopub.execute_input":"2024-08-19T14:27:30.267888Z","iopub.status.idle":"2024-08-19T14:27:30.273603Z","shell.execute_reply.started":"2024-08-19T14:27:30.267853Z","shell.execute_reply":"2024-08-19T14:27:30.272738Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import yaml\nimport os\n\ncwd = '/kaggle/working/'\n\n\ndata = dict(\n    path  = cwd,\n    train =  '../input/ip102-yolov5/IP102_YOLOv5/images/train' ,\n    val   =  '../input/ip102-yolov5/IP102_YOLOv5/images/val',\n    nc    = 32,\n    names =  ['rice leaf roller', 'rice leaf caterpillar', 'paddy stem maggot', 'asiatic rice borer', 'yellow rice borer',\n        'rice gall midge', 'Rice Stemfly', 'brown plant hopper', 'white backed plant hopper', 'small brown plant hopper',\n        'rice water weevil', 'rice leafhopper', 'grain spreader thrips', 'rice shell pest', 'grub', 'mole cricket', 'wireworm',\n        'white margined moth', 'black cutworm', 'large cutworm', 'yellow cutworm', 'red spider', 'corn borer', 'army worm', 'aphids',\n        'Potosiabre vitarsis', 'peach borer', 'english grain aphid', 'green bug', 'bird cherry-oataphid', 'wheat blossom midge',\n        'penthaleus major', 'longlegged spider mite', 'wheat phloeothrips', 'wheat sawfly', 'cerodonta denticornis', 'beet fly',\n        'flea beetle', 'cabbage army worm', 'beet army worm', 'Beet spot flies', 'meadow moth', 'beet weevil', 'sericaorient alismots chulsky',\n        'alfalfa weevil', 'flax budworm', 'alfalfa plant bug', 'tarnished plant bug', 'Locustoidea', 'lytta polita', 'legume blister beetle',\n        'blister beetle', 'therioaphis maculata Buckton', 'odontothrips loti', 'Thrips', 'alfalfa seed chalcid', 'Pieris canidia',\n        'Apolygus lucorum', 'Limacodidae', 'Viteus vitifoliae', 'Colomerus vitis', 'Brevipoalpus lewisi McGregor', 'oides decempunctata',\n        'Polyphagotars onemus latus', 'Pseudococcus comstocki Kuwana', 'parathrene regalis', 'Ampelophaga', 'Lycorma delicatula', 'Xylotrechus',\n        'Cicadella viridis', 'Miridae', 'Trialeurodes vaporariorum', 'Erythroneura apicalis', 'Papilio xuthus', 'Panonchus citri McGregor',\n        'Phyllocoptes oleiverus ashmead', 'Icerya purchasi Maskell', 'Unaspis yanonensis', 'Ceroplastes rubens', 'Chrysomphalus aonidum',\n        'Parlatoria zizyphus Lucus', 'Nipaecoccus vastalor', 'Aleurocanthus spiniferus', 'Tetradacus c Bactrocera minax ', 'Dacus dorsalis(Hendel)',\n        'Bactrocera tsuneonis', 'Prodenia litura', 'Adristyrannus', 'Phyllocnistis citrella Stainton', 'Toxoptera citricidus', 'Toxoptera aurantii',\n        'Aphis citricola Vander Goot', 'Scirtothrips dorsalis Hood', 'Dasineura sp', 'Lawana imitata Melichar', 'Salurnis marginella Guerr',\n        'Deporaus marginatus Pascoe', 'Chlumetia transversa', 'Mango flat beak leafhopper', 'Rhytidodera bowrinii white', 'Sternochetus frigidus',\n        'Cicadellidae']\n    )\n\nwith open(os.path.join( cwd , 'IP102.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n\nf = open(os.path.join( cwd , 'IP102.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:40:30.895899Z","iopub.execute_input":"2024-08-19T14:40:30.896866Z","iopub.status.idle":"2024-08-19T14:40:30.917575Z","shell.execute_reply.started":"2024-08-19T14:40:30.896825Z","shell.execute_reply":"2024-08-19T14:40:30.91656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python train.py --img 128 --batch 32 --epochs 2 --data \"/kaggle/working/IP102.yaml\" --weights yolov5s.pt --project \"insect_Image Classification\"  --name 'Image Classification'  --save-period 1 --bbox_interval 1 --cache","metadata":{"execution":{"iopub.status.busy":"2024-08-19T14:40:31.863897Z","iopub.execute_input":"2024-08-19T14:40:31.864276Z","iopub.status.idle":"2024-08-19T14:47:38.267611Z","shell.execute_reply.started":"2024-08-19T14:40:31.864247Z","shell.execute_reply":"2024-08-19T14:47:38.266398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\nfrom IPython.display import Image, display\n\nimage_path_pattern = '/kaggle/working/yolov5/insect_Image Classification/Image Classification/*.jpg'\n\nfor imageName in glob.glob(image_path_pattern):\n    display(Image(filename=imageName))\n    print(f\"Filename: {imageName.split('/')[-1]}\")\n    print(\"\\n\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-19T15:03:19.198277Z","iopub.execute_input":"2024-08-19T15:03:19.198932Z","iopub.status.idle":"2024-08-19T15:03:19.246756Z","shell.execute_reply.started":"2024-08-19T15:03:19.198902Z","shell.execute_reply":"2024-08-19T15:03:19.245828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Step 7: Deployment**","metadata":{"id":"bY0xs7rtlnBQ"}},{"cell_type":"markdown","source":"### Deploying a Trained Object Detection Model\n\n**Deployment Options**\n\n**a. Cloud-based Deployment:** It refers to hosting and running applications, services, or models on cloud infrastructure rather than on local servers or hardware.\n   - **Pros:**\n     - **Scalability:** Easily handle high traffic by scaling resources.\n     - **Managed Services:** Use services like AWS SageMaker, Google AI Platform, or Azure Machine Learning which provide infrastructure management.\n     - **Integration:** Easy integration with other cloud services like storage and databases.\n   - **Cons:**\n     - **Cost:** Can become expensive with high usage.\n     - **Latency:** May introduce latency due to network communication.\n     - **Data Privacy:** Sensitive data may be exposed if not managed properly.\n     \n**b. On-Premises Deployment:** It refers to the practice of hosting and managing applications, services, or models on hardware and infrastructure located within an organizationâ€™s own premises.\n   - **Pros:**\n     - **Control:** Full control over the hardware and software environment.\n     - **Latency:** Lower latency as the model runs locally.\n     - **Data Privacy:** Better for handling sensitive data within a secure environment.\n   - **Cons:**\n     - **Scalability:** Limited by available hardware; scaling requires additional investment.\n     - **Maintenance:** Requires in-house expertise for maintenance and updates.\n     - **Cost:** Upfront costs for hardware and setup.\n\n**c. Edge Deployment:** It refers to running applications, services, or models on local devices or computing resources that are situated closer to where data is generated or used, rather than in a centralized data center or cloud. \n   - **Pros:**\n     - **Real-Time Processing:** Low latency as processing happens on the device.\n     - **Reduced Bandwidth:** Less need to send data to the cloud, saving bandwidth.\n     - **Offline Capabilities:** Can operate without an internet connection.\n   - **Cons:**\n     - **Hardware Constraints:** Limited by the computational power of the edge device.\n     - **Updates:** More challenging to update models across multiple devices.\n     - **Data Privacy:** While data remains local, security concerns still exist.\n\n**d. Hybrid Deployment:** It combines both on-premises infrastructure and cloud-based services. This approach allows organizations to use local servers and systems for certain applications or data, while leveraging cloud resources for other tasks or to handle variable workloads. \n   - **Pros:**\n     - **Flexibility:** Combine cloud, on-premises, and edge for optimized performance and cost.\n     - **Redundancy:** Failover solutions can be implemented.\n     - **Scalability and Privacy:** Balance between cloud scalability and on-premises privacy.\n   - **Cons:**\n     - **Complexity:** More complex to manage and configure.\n     - **Integration:** Ensuring seamless integration between different environments can be challenging.\n\n**Recommended Deployment:** Edge Deployment is the best choice for object detection models due to the need for real-time processing and low latency.\n\n**Steps to Monitor and Ensure Model Performance**\n\n**a. Establish Monitoring Metrics** :Track metrics like precision, recall, F1-score, inference time, and accuracy.\n\n**b. Regular Model Evaluation** :Periodically evaluate the model using a validation set to check for concept drift. And also re-train the model as needed with updated data to maintain performance.\n\n**c. User Feedback** :Collect feedback from end-users to identify potential issues or areas for improvement.\n","metadata":{}}]}